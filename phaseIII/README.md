# Phase III: Prototypes and User Testing

## Introduction

Phase III's sole purpose was to test the Rosetta Song prototype with a user test. By conducting user tests, our team aims to identify user preferences, find problematic aspects of the prototype, and note opportunities for improvement in the prototype's design and functionality.
## Draft Protocol
[Draft Protocol](../phaseIII/x18_Draft_protocol.pdf)
## Methods
Our team utilized all our planning in order to conduct the user testing of our prototype. This included our introduction, in which we explained to the tester the atmosphere of the test, how the test’s information will be kept and how we would like the tester to behave during the testing. Next we wanted some background information on each subject so we gathered information that was non specific to the participants. Reference the Draft protocol for specific questions. The goal of the initial questions were to establish how possibly useful our product would be but also if the participants had any general interest in the problem our product is meant to solve. After we gathered some basic information about our participants' background we moved onto conducting our first task. In this task we had the user demonstrate how they would act if they were to run into an error on our site. The goal of this task was to understand if our system’s error response is easy and understandable. The next task had the participants updating information on the profile page. The purpose of this task was to see how the user navigated the site and if the user could get to the correct pages. For the third task our team had our participants attempt to find relevant information to their account. The purpose of this task was to make sure that the stats that our site renders are not only useful but wanted. Reference the draft protocol for any specific language. Finally we had our participants conduct a simple transfer of music from one service to another. This was to test the main functionality of the website and see if the user could understand our conventions. After finishing the tasks each participant was asked a series of questions to debrief the subject from the experience they had. These questions included topics like what the participant enjoyed and did not enjoy, if the error messages were understandable and dealt with the issue correctly, what kind of things could be optimized inside the tasks and more. Reference the draft protocol for any specific questions. The purpose of these questions were to find out if the participants had any more information about their experience and how we could improve. To close the test we thanked the participants and dismissed them.

## Findings

All of our tests had extremely high ratings from the partisipants in terms of statisfaction (greater than 4.5/5 average) except for our task 1, which had an average of 3.86/5. When taking an average of all task ratings, the average came out to exactly 4.5/5 across all task, and the average standard deviation was 0.903. Task 1 and task 3 were the only tasks to specifically be given criticism in the debrief, but task 1 much more so. A majority of participants either listed their least favorite part of the experience as the "transfer cart" or the "profile section". Task 1 also had the highest standard deviation at 1.35, and had by far the most amount of variation in scores, along with task 3, which had the second highest standard deviation at 1.133.  Task 2 and 4 both had a satisfaction rate of over 4.7/5 and a standard deviation of under 0.8. Every task was completed in under 5 minutes, and many participants completed tasks in under 1 minute, making the average time under 3 minutes per task. Every task was completed by every participant, except for one time in task 1, and one time in task 3, who were then given a hint and were able to complete the task (still marked as failure).

## Conclusions

As shown by our data, task 1 appeared to be the most challenging for the participants. This task requires the participant to use the main feature of our product, the playlist transferer. One participant seemed to get lost through the menus, eventually looping back and repeating a cycle that did not make progress toward the task. The two main points we took away from that task were the misunderstandings of the word "cart" and the possibility of having too many buttons.
Overall though, our findings show that the most common concern from participants was the ambiguity of the  "transfer cart" idea. In our prototype, the transfer cart is displayed as a small button at the top that says "cart". Some participants noted that it led them to believe they were purchasing something, and was the reason they did not click on it during the tasks. Looking back this makes sense, as it is a common convention for a cart to be related to purchasing. To follow up on that, we have pondered alternatives such as labeling the button "transfer cart" instead of just "cart". Additionally, dropping the "cart" entirely would resolve this issue. Issues with the profile button were less mentioned but were still addressed. One concern was that it was easy to miss and the participant suggested adding the name to the profile button. We thought this was a good idea as it would reinforce and draw attention to that aspect.

## Caveats

All participants of the study are college aged usability students, so they potentially may have interacted with the prototype differently than a random subset of participants would have. Each participant took part in multiple user tests, so the later participants may have been fatigued from the previous studies. The Rosetta Song study had seven participants, so the impact of a small sample size could have had an effect on the results of the study.
